\documentclass{article}

\usepackage{mathtools}
% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

% \usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Tracking Objects}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
	Sebastian Damrich 
	%\thanks{Use footnote for providing further
    %information about author (webpage, alternative
    %address)---\emph{not} for acknowledging funding agencies.} 
    \\
  Heidelberg University\\
  \texttt{sebastian.damrich@gmx.de} \\
  %% examples of more authors
   \And
   Lukas Bold  \\
  %% Affiliation \\
   Heidelberg University \\
  \texttt{lukasbold92@gmail.com} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch
  (3~picas) onboth the left- and right-hand margins. Use 10~point
  type, with a vertical spacing (leading) of 11~points.  The word
  \textbf{Abstract} must be centered, bold, and in point size 12. Two
  line spaces precede the abstract. The abstract must be limited to
  one paragraph.
\end{abstract}

\section{Differentiate through KKT Conditions}
\label{others}

The OptNet Paper introduced a method to differentiate through a quadratic problem given by
\[ \min_f \frac{1}{2} f^T Q f + c^T f \]
\[\text{subject to }~  Af-b=0 ~\text{ and }~ Gf \leq h, \]

where $f\in \mathbb{R}^n$ is the optimization variable (in our case the flow), $Q\in \mathbb{R}^{n\times n}$ is a positive semidefinite matrix, $c\in \mathbb{R}^n$, $A\in \mathbb{R}^{m\times n}$, $b\in \mathbb{R}^m$, $G\in \mathbb{R}^{p\times n}$ and $h\in \mathbb{R}^p$ are the data describing the minimization problem. 

Since our first purpose was the application of this process to a minimum-cost flow problem (MCFP) for computing a shortest path through a graph associated to a video, we could restrict ourselves to a linear optimization problem of the following form:

\[\min_f c^T f\] 
\[ \text{subject to} ~~Af=b,~f\geq 0 ~\text{and}~ f \leq \kappa,\]

where $f$ is the flow through the video, $c$ is the cost vector and $\kappa$ is the capacity vector related the graph's edges. Together with suitable $A$, $b$ and the inequalities $f\geq 0$, $f\leq \kappa$, this case encodes the MCFP. Now we pursue the OptNet Paper and use the definitions
\[G\coloneqq \begin{pmatrix} -I \\  I \end{pmatrix} ~~\text{and} ~~ h\coloneqq \begin{pmatrix} 0 \\ \kappa \end{pmatrix} \]

to get the both restrictions $Af=b$ and $Gf\leq h$. Then we obtain the Lagrangian function
\[L(f, \nu, \lambda) = c^T f + \nu ^T (Af-b) + \lambda^T(Gf-\kappa)\]
which leads us to the KKT conditions

\begin{align*}
\nabla_f L(f^*,\nu^*, \lambda^*) = c + A^T \nu^*  + G^T\lambda^* &= 0 \\
\nabla _\nu L(f^*, \nu^*, \lambda^*) = Af^* - b &= 0 \\
D(\lambda^*) (\nabla_\lambda L(f^*, \nu^*, \lambda^*)) = D(\lambda^*)(Gf^*-\kappa) &= 0
\end{align*}

where D(-) creates a diagonal matrix consisting of the entries of a vector and $f^*$, $\nu^*$ and $\lambda^*$ are the optimal primal and dual variables. After taking the differentials of the KKT conditions we receive 

\begin{align*}
\mathrm{d}c + \mathrm{d}A^T \nu^* + A^T \mathrm{d}\nu + \mathrm{d}G^T \lambda^* + G^T\mathrm{d}\lambda &= 0 \\
\mathrm{d} A f^* + A\mathrm{d}f - \mathrm{d}b &=0 \\
D(Gf^*- h) \mathrm{d}\lambda + D(\lambda^*)(\mathrm{d}Gf^* + G\mathrm{d} f - \mathrm{d}h) &=0.
\end{align*}
The following matrix form is equivalent:

\begin{align}
\begin{pmatrix} 0 & G^T & A^T \\ D(\lambda^*) G & D(Gf^* -h) & 0 \\ A & 0 & 0 \end{pmatrix}
\begin{pmatrix} \mathrm{d} f \\ \mathrm{d} \lambda \\ \mathrm{d} \nu \end{pmatrix} = 
\begin{pmatrix} -\mathrm{d}c - \mathrm{d}G^T \lambda^* -\mathrm{d}A^T\nu^* \\ 
-D(\lambda^*)\mathrm d G f^* + D(\lambda^*)\mathrm d h \\
-\mathrm d A f^* + \mathrm d b \end{pmatrix} \label{diff equation}
\end{align}
where the left hand matrix is from now on defined by $M$.
In the next step we are interested in the derivation $\frac{\partial \ell}{\partial c}\in \mathbb R ^n$, where $\ell: \mathbb{R}^n \rightarrow \mathbb R,~f\mapsto \ell(f) $ is our loss function. So we want to know how the loss vary with respect to the costs of the graph. Later we also discuss the approach of considering $\frac{\partial \ell}{\partial h}\in \mathbb R^n$, where $h$ carries the information about the capacities $\kappa$. Now we differentiate the equation (\ref*{diff equation}) by $\partial c$ and obtain

\[
M \begin{pmatrix} \frac{\partial f}{\partial c} \\[4pt] \frac{\partial\lambda}{\partial c} \\[4pt] \frac{\partial \nu}{\partial c} \end{pmatrix}
= 
\begin{pmatrix} -I \\ 0 \\ 0 \end{pmatrix}.
\]

In our case the top left entry of $M$ is the zero matrix in contrast to the positive definite matrix $Q$ in the OptNet paper. Therefore the the two issues of existence and uniqueness of a solution occur because the matrix $M$ is not invertible. However we can continue and try to compute a solution (if it exists), which is of course not unique. With this solution for $\frac{\partial f}{\partial c}$ we could compute the desired derivation

\[
\frac{\partial \ell }{\partial c} = \frac{\partial \ell}{\partial f} \frac{\partial f}{\partial c}~
\]

to insert it as the gradient for the Neural Network. 

\subsection{Theoretical issues}

The first issue was already mentioned above. 


\end{document}
